{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_names:['roses', 'sunflowers', 'daisy', 'dandelion', 'tulips', 'LICENSE.txt']\n"
     ]
    }
   ],
   "source": [
    "sources_path = \"../CoreML/flower_photos/\"\n",
    "split_rate = 0.2\n",
    "categories = glob.glob(sources_path + \"*\")\n",
    "category_names = [x.split(\"/\")[-1] for x in categories]\n",
    "categories_count = len(category_names)\n",
    "print \"category_names:%s\"%category_names\n",
    "log_path = \"logs/\"\n",
    "checkpoint_path = \"logs/model.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本地文件读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    train_image_paths = []\n",
    "    test_image_paths = []\n",
    "    for path in categories:\n",
    "        image_paths = glob.glob(path + \"/*.jpg\")\n",
    "        split_count = int(len(image_paths) * (1 - split_rate))\n",
    "        train_image_paths += image_paths[:split_count]\n",
    "        test_image_paths += image_paths[split_count:]\n",
    "\n",
    "    train_labels = [x.split(\"/\")[-2] for x in train_image_paths]\n",
    "    test_labels = [x.split(\"/\")[-2] for x in test_image_paths]\n",
    "    train_label_list = [category_names.index(x) for x in train_labels]\n",
    "    test_labels_list = [category_names.index(x) for x in test_labels]\n",
    "\n",
    "    train_image_paths = np.asarray(train_image_paths)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    test_image_paths = np.asarray(test_image_paths)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "\n",
    "    return (train_image_paths, train_label_list), (test_image_paths, test_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图片预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_images(images,labels,count):\n",
    "    with tf.Session() as sess:\n",
    "        i=0\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord = coord)\n",
    "        try:\n",
    "            while not coord.should_stop() and i<1:\n",
    "                for j in np.arange(count):\n",
    "                    plt.imshow(images[j,:,:,:])\n",
    "                    index = tf.argmax(labels[j])                  \n",
    "                    plt.xlabel(category_names[sess.run(index)])\n",
    "                    plt.show()\n",
    "                i+=1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done!')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器，用于生产训练数据\n",
    "- Returns:训练需要的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(image,label):\n",
    "    \n",
    "    image_contents = tf.read_file(image)\n",
    "    image = tf.image.decode_jpeg(image_contents, channels=3)\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image,32,32)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    label = tf.one_hot(label,depth=categories_count)\n",
    "    label = tf.cast(label,dtype=tf.uint8)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(image, label, image_W, image_H, batch_size):\n",
    "    \n",
    "    image_tf = tf.cast(image, tf.string)\n",
    "    label_tf = tf.cast(label, tf.int32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_tf,label_tf))\n",
    "    dataset = dataset.shuffle(buffer_size=len(label))\n",
    "    dataset = dataset.map(parse_data)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator =  dataset.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x = tf.placeholder(\"float\",shape=[None,32,32,3])\n",
    "y = tf.placeholder(\"float\",shape=[None,categories_count])\n",
    "\n",
    "batch_size = 50;\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, batch_size, n_classes):\n",
    "    \n",
    "    ## 第一层卷积 \n",
    "    w_conv1 = tf.Variable(tf.truncated_normal([3,3,3,16],stddev=0.1))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1,shape=[16]))\n",
    "    y_conv1 = tf.nn.conv2d(images,w_conv1,strides=[1,1,1,1], padding=\"SAME\") + b_conv1\n",
    "    h_conv1 = tf.nn.relu(y_conv1)\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,3,3,1],strides=[1,2,2,1], padding=\"SAME\")\n",
    "    \n",
    "    ## 第二层卷积\n",
    "    w_conv2 = tf.Variable(tf.truncated_normal([3,3,16,128],stddev=0.1))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1,shape=[128]))\n",
    "    y_conv2 = tf.nn.conv2d(h_pool1,w_conv2,strides=[1,1,1,1], padding=\"SAME\") + b_conv2\n",
    "    h_conv2= tf.nn.relu(y_conv2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2,ksize=[1,3,3,1],strides=[1,1,1,1], padding=\"SAME\")\n",
    "    h_pool2_flat = tf.reshape(h_pool2,[batch_size,-1])\n",
    "    \n",
    "    ## 全连接层    \n",
    "    w_fc1 = tf.Variable(tf.truncated_normal([16*16*128,128],stddev=0.005))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1,shape=[128]))\n",
    "    y_fc1 = tf.matmul(h_pool2_flat, w_fc1) + b_fc1\n",
    "    h_fc1 = tf.nn.relu(y_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "    \n",
    "    ## 全连接层\n",
    "    w_fc2 = tf.Variable(tf.truncated_normal([128,128],stddev=0.005))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.005,shape=[128]))\n",
    "    y_fc2 = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n",
    "    h_fc2= tf.nn.relu(y_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "    \n",
    "    ## 全连接输出层\n",
    "    w_fc3 = tf.Variable(tf.truncated_normal([128,n_classes],stddev=0.005))\n",
    "    b_fc3 = tf.Variable(tf.constant(0.005,shape=[n_classes]))\n",
    "    y_fc3 = tf.matmul(h_fc2_drop, w_fc3) + b_fc3\n",
    "    \n",
    "    return y_fc3\n",
    "\n",
    "\n",
    "def losses(logits, labels):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2 \\\n",
    "            (logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name + '/loss', loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def trainning(loss, learning_rate):\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        labels = tf.map_fn(lambda x:tf.argmax(x) ,labels)\n",
    "        labels = tf.cast(label,dtype=tf.int32)\n",
    "        print labels\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name + '/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"accuracy/Cast/x:0\", shape=(2934,), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 50 and 2934 for 'accuracy/in_top_k/InTopKV2' (op: 'InTopKV2') with input shapes: [50,6], [2934], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-360b4e653067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msummary_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-421daa262828>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36min_top_k\u001b[0;34m(predictions, targets, k, name)\u001b[0m\n\u001b[1;32m   2676\u001b[0m   \"\"\"\n\u001b[1;32m   2677\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"in_top_k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_top_kv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36min_top_kv2\u001b[0;34m(predictions, targets, k, name)\u001b[0m\n\u001b[1;32m   4227\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4228\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4229\u001b[0;31m         \"InTopKV2\", predictions=predictions, targets=targets, k=k, name=name)\n\u001b[0m\u001b[1;32m   4230\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4231\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 50 and 2934 for 'accuracy/in_top_k/InTopKV2' (op: 'InTopKV2') with input shapes: [50,6], [2934], []."
     ]
    }
   ],
   "source": [
    "(train, label), (test, testLabel) = readData()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    iterator = get_batch(train,label,32,32,batch_size)\n",
    "    train_logits = inference(x, batch_size, categories_count)\n",
    "    train_loss = losses(train_logits, y)\n",
    "    train_op = trainning(train_loss, learning_rate)\n",
    "    train_acc = evaluation(train_logits, y)\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        for step in range(2000):\n",
    "            batch = iterator.get_next() \n",
    "            _, tra_loss, tra_acc = sess.run([train_op, train_loss, train_acc], feed_dict = {x:batch[0], y:batch[1], keep_prob:0.5})\n",
    "            if step % 50 == 0:\n",
    "                print('Step %d,train loss = %.2f,train occuracy = %.2f%%' % (step, tra_loss, tra_acc))\n",
    "                summary_str = sess.run(summary_op)\n",
    "                train_writer.add_summary(summary_str, step)\n",
    "\n",
    "            if step % 2000 == 0 or (step + 1) == MAX_STEP:\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training epoch limit reached')\n",
    "    finally:\n",
    "        print('Done!')\n",
    "        \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
